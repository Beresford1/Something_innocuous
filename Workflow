# So here I'll outline my workflow as it's happening

# 1) Downloaded the data from Fugu to FASTDATA on iceberg using wget command

# 2) Unzipped the four base files individually using: gunzip <filename>
# 	 Here there were 2 log files and 2 files with our raw data, also zipped, and which also contained the FastQC files generated
#    by Fugu.

# 3) Unzipped all of the raw read zip files using: gunzip *.gz
# 	 For each library (individual) there are 4 lanes with forward and reverse reads, and the run was duplicated. This left us
# 	 with 16 files per individual library. Each of these files can be up to a GB in size (memory), but the average seems to be
#	   around 0.5 of a GB. There are ~600 files per run, including the unidentified reads from each lane/run.

# 4) After checking out the FastQC files generated by Fugu, it was apparent that the barcodes & adaptors had all been removed
#    following their demultiplexing; this is the process of dividing the reads up into idividual libraries according to barcodes.

# 5) The next step was to Quality filter the files, so I have adapted a .py script written for me by Henry to submit batch
#    jobs via the sungrid engine on Iceberg, see: quality_filtering.py -> using the fastx - toolkit programs.
#    I generated the list file for this script using: ls *.fastq > fastq_list.txt

# 6) Next I wanted to check each part of the submissions script, so I omitted each part individually and checked the output.
#    To see this process for yourself, see the development of the script's various versions. In addition to this, I considered
#    concatenating the individual lane files for forward & reverse reads into the same file before QC, but decided against this
#    for numerous reasons:
#    a) Each file would be larger and take longer to process individually by iceberg, and may increase the base memeory
#       requirements, which would also make the queing take longer since smaller jobs que quicker.
#    b) The submissions script isn't affected by the number of files it has to submit to Iceberg, since it can submit up to 2000
#       jobs, with 200 running simultaneously.
#    c) I have no idea whether concatenating the files would result in a loss of functionality of the filtering program, or if
#       by merging files, you lose some of the information. Also I would want to go back and compare both methods with repeats,
#       which I don't want to do.

